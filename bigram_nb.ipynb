{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in dataset: 32033\n",
      "max word length: 15\n",
      "min word length: 2\n",
      "unique characters in dataset: 27\n",
      "vocabulary:\n",
      "<eos> a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "example for a word:\n",
      "emma\n"
     ]
    }
   ],
   "source": [
    "def load_data(path: str) -> Tuple[List[str], List[str]]:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    words = data.splitlines()\n",
    "    words = [word.strip() for word in words] # Remove leading/trailing whitespace\n",
    "    words = [word for word in words if word] # Remove empty strings\n",
    "\n",
    "    vocab = sorted(list(set(''.join(words))))\n",
    "    vocab = ['<eos>'] + vocab\n",
    "    print(f\"number of examples in dataset: {len(words)}\")\n",
    "    print(f\"max word length: {max([len(word) for word in words])}\")\n",
    "    print(f\"min word length: {min([len(word) for word in words])}\")\n",
    "    print(f\"unique characters in dataset: {len(vocab)}\")\n",
    "    print(\"vocabulary:\")\n",
    "    print(' '.join(vocab))\n",
    "    print('example for a word:')\n",
    "    print(words[0])\n",
    "    return words, vocab\n",
    "\n",
    "words, vocab = load_data('names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: emma\n",
      "encoded: [0, 5, 13, 13, 1, 0]\n",
      "decoded: <eos>emma<eos>\n",
      "\n",
      "word: olivia\n",
      "encoded: [0, 15, 12, 9, 22, 9, 1, 0]\n",
      "decoded: <eos>olivia<eos>\n",
      "\n",
      "word: ava\n",
      "encoded: [0, 1, 22, 1, 0]\n",
      "decoded: <eos>ava<eos>\n",
      "\n",
      "word: isabella\n",
      "encoded: [0, 9, 19, 1, 2, 5, 12, 12, 1, 0]\n",
      "decoded: <eos>isabella<eos>\n",
      "\n",
      "word: sophia\n",
      "encoded: [0, 19, 15, 16, 8, 9, 1, 0]\n",
      "decoded: <eos>sophia<eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(word: str, vocab: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Encode a word, add <eos> at the beginning and the end of the word.\n",
    "    \"\"\"\n",
    "    return [vocab.index('<eos>')] + [vocab.index(char) for char in word] + [vocab.index('<eos>')]\n",
    "\n",
    "def decode(indices: List[int], vocab: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Decode a list of indices to a word using the vocabulary.\n",
    "    \"\"\"\n",
    "    return ''.join([vocab[index] for index in indices])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"word: {words[i]}\")\n",
    "    print(f\"encoded: {encode(words[i], vocab)}\")\n",
    "    print(f\"decoded: {decode(encode(words[i], vocab), vocab)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 13, 13, 1, 0]\n",
      "<eos>emma<eos>\n",
      "32033\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "encoded_words = [encode(word, vocab) for word in words]\n",
    "print(encoded_words[0])\n",
    "print(decode(encoded_words[0], vocab))\n",
    "print(len(encoded_words))\n",
    "print(len(encoded_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(encoded_words: List[List[int]]) -> Tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"\n",
    "    Convert a list of encoded words to a list of bigrams.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for word in encoded_words:\n",
    "        for char1, char2 in zip(word[:-1], word[1:]):\n",
    "            X.append(char1)\n",
    "            y.append(char2)\n",
    "    return jax.numpy.array(X), jax.numpy.array(y)\n",
    "\n",
    "def get_train_val_test(encoded_words: List[List[int]]) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation and test sets.\n",
    "    \"\"\"\n",
    "    random.shuffle(encoded_words)\n",
    "    train_words = encoded_words[:int(0.8*len(encoded_words))]\n",
    "    val_words = encoded_words[int(0.8*len(encoded_words)):int(0.9*len(encoded_words))]\n",
    "    test_words = encoded_words[int(0.9*len(encoded_words)):]\n",
    "    X_train, y_train = get_dataset(train_words)\n",
    "    X_val, y_val = get_dataset(val_words)\n",
    "    X_test, y_test = get_dataset(test_words)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_train_val_test(encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182466,), (182466,), (22838,), (22838,), (22842,), (22842,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "W = np.random.randn(len(vocab), EMBEDDING_DIM)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.86328125, -1.015625  , -0.8359375 ,  0.26953125,  0.01367188,\n",
       "        0.12109375,  0.51953125, -0.671875  ,  1.234375  , -0.0546875 ],      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jax.nn.one_hot(X_train, len(vocab)) @ W)[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86347077, -1.01740871, -0.83740075,  0.26996182,  0.01365725,\n",
       "        0.12102629,  0.5200092 , -0.67272471,  1.23825448, -0.05461122])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[X_train][0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0,  7, 15, ...,  5, 13, 25], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
