{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, NamedTuple, Tuple\n",
    "\n",
    "import jax\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in dataset: 32033\n",
      "max word length: 15\n",
      "min word length: 2\n",
      "unique characters in dataset: 27\n",
      "vocabulary:\n",
      "<eos> a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "example for a word:\n",
      "emma\n"
     ]
    }
   ],
   "source": [
    "def load_data(path: str) -> Tuple[List[str], List[str]]:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    words = data.splitlines()\n",
    "    words = [word.strip() for word in words] # Remove leading/trailing whitespace\n",
    "    words = [word for word in words if word] # Remove empty strings\n",
    "\n",
    "    vocab = sorted(list(set(''.join(words))))\n",
    "    vocab = ['<eos>'] + vocab\n",
    "    print(f\"number of examples in dataset: {len(words)}\")\n",
    "    print(f\"max word length: {max([len(word) for word in words])}\")\n",
    "    print(f\"min word length: {min([len(word) for word in words])}\")\n",
    "    print(f\"unique characters in dataset: {len(vocab)}\")\n",
    "    print(\"vocabulary:\")\n",
    "    print(' '.join(vocab))\n",
    "    print('example for a word:')\n",
    "    print(words[0])\n",
    "    return words, vocab\n",
    "\n",
    "words, vocab = load_data('names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: emma\n",
      "encoded: [0, 5, 13, 13, 1, 0]\n",
      "decoded: <eos>emma<eos>\n",
      "\n",
      "word: olivia\n",
      "encoded: [0, 15, 12, 9, 22, 9, 1, 0]\n",
      "decoded: <eos>olivia<eos>\n",
      "\n",
      "word: ava\n",
      "encoded: [0, 1, 22, 1, 0]\n",
      "decoded: <eos>ava<eos>\n",
      "\n",
      "word: isabella\n",
      "encoded: [0, 9, 19, 1, 2, 5, 12, 12, 1, 0]\n",
      "decoded: <eos>isabella<eos>\n",
      "\n",
      "word: sophia\n",
      "encoded: [0, 19, 15, 16, 8, 9, 1, 0]\n",
      "decoded: <eos>sophia<eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(word: str, vocab: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Encode a word, add <eos> at the beginning and the end of the word.\n",
    "    \"\"\"\n",
    "    return [vocab.index('<eos>')] + [vocab.index(char) for char in word] + [vocab.index('<eos>')]\n",
    "\n",
    "def decode(indices: List[int], vocab: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Decode a list of indices to a word using the vocabulary.\n",
    "    \"\"\"\n",
    "    return ''.join([vocab[index] for index in indices])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"word: {words[i]}\")\n",
    "    print(f\"encoded: {encode(words[i], vocab)}\")\n",
    "    print(f\"decoded: {decode(encode(words[i], vocab), vocab)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 13, 13, 1, 0]\n",
      "<eos>emma<eos>\n",
      "32033\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "encoded_words = [encode(word, vocab) for word in words]\n",
    "print(encoded_words[0])\n",
    "print(decode(encoded_words[0], vocab))\n",
    "print(len(encoded_words))\n",
    "print(len(encoded_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(encoded_words: List[List[int]]) -> Tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"\n",
    "    Convert a list of encoded words to a list of bigrams.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for word in encoded_words:\n",
    "        for char1, char2 in zip(word[:-1], word[1:]):\n",
    "            X.append(char1)\n",
    "            y.append(char2)\n",
    "    return jax.numpy.array(X), jax.numpy.array(y)\n",
    "\n",
    "def get_train_val_test(encoded_words: List[List[int]]) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation and test sets.\n",
    "    \"\"\"\n",
    "    random.shuffle(encoded_words)\n",
    "    train_words = encoded_words[:int(0.8*len(encoded_words))]\n",
    "    val_words = encoded_words[int(0.8*len(encoded_words)):int(0.9*len(encoded_words))]\n",
    "    test_words = encoded_words[int(0.9*len(encoded_words)):]\n",
    "    X_train, y_train = get_dataset(train_words)\n",
    "    X_val, y_val = get_dataset(val_words)\n",
    "    X_test, y_test = get_dataset(test_words)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_train_val_test(encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182581,), (182581,), (22787,), (22787,), (22778,), (22778,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weights(NamedTuple):\n",
    "    W: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights() -> Weights:\n",
    "    return Weights(W=np.random.randn(len(vocab), len(vocab)))\n",
    "\n",
    "def forward(weights: Weights, X: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    1) index into the weights matrix W using the input indices\n",
    "    2) apply the softmax function to obtain a probability distribution over the next character.\n",
    "    \"\"\"\n",
    "    logits = weights.W[X]\n",
    "    exp_logits = jax.numpy.exp(logits)\n",
    "    probs = exp_logits / jax.numpy.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def loss(weights: Weights, X: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    1) get the probabilities for the next character\n",
    "    2) index into the probabilities using the true next character\n",
    "    3) take the negative log of the probability\n",
    "    4) return the mean loss over all the examples\n",
    "    \"\"\"\n",
    "    probs = forward(weights, X)\n",
    "    return -jax.numpy.log(probs[jax.numpy.arange(len(y)), y]).mean()\n",
    "\n",
    "def update(weights: Weights, X: jax.Array, y: jax.Array, learning_rate: float) -> Weights:\n",
    "    \"\"\"\n",
    "    1) get the probabilities for the next character\n",
    "    2) compute the gradient of the loss with respect to the weights\n",
    "    3) update the weights using the gradient\n",
    "    \"\"\"\n",
    "    grads = jax.grad(loss)(weights, X, y)\n",
    "    return jax.tree.map(lambda w, g: w - learning_rate * g, weights, grads)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(weights: Weights, X: jax.Array, y: jax.Array, learning_rate: float) -> Tuple[Weights, float]:\n",
    "    \"\"\"\n",
    "    1) compute the loss\n",
    "    2) compute the gradient of the loss with respect to the weights\n",
    "    3) update the weights using the gradient\n",
    "    4) return the updated weights and the loss\n",
    "    \"\"\"\n",
    "    loss_value = loss(weights, X, y)\n",
    "    weights = update(weights, X, y, learning_rate)\n",
    "    return weights, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 3.7597320079803467, val_loss: 3.4014439582824707\n",
      "epoch: 10, loss: 2.6564254760742188, val_loss: 2.650994300842285\n",
      "epoch: 20, loss: 2.5583841800689697, val_loss: 2.563549518585205\n",
      "epoch: 30, loss: 2.524369239807129, val_loss: 2.531252384185791\n",
      "epoch: 40, loss: 2.5066328048706055, val_loss: 2.514086961746216\n",
      "epoch: 50, loss: 2.4957680702209473, val_loss: 2.503443956375122\n",
      "epoch: 60, loss: 2.4884531497955322, val_loss: 2.496213674545288\n",
      "epoch: 70, loss: 2.4831907749176025, val_loss: 2.490983486175537\n",
      "epoch: 80, loss: 2.4792237281799316, val_loss: 2.487029790878296\n",
      "epoch: 90, loss: 2.4761250019073486, val_loss: 2.4839417934417725\n"
     ]
    }
   ],
   "source": [
    "weights = init_weights()\n",
    "N_EPOCHS = 100\n",
    "LR = 50\n",
    "for epoch in range(N_EPOCHS):\n",
    "    weights, loss_value = train_step(weights, X_train, y_train, LR)\n",
    "    val_loss = loss(weights, X_val, y_val)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch: {epoch}, loss: {loss_value}, val_loss: {val_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
